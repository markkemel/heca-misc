Hecatonchire Sanity Tests
=========================
We hace a set of sanity tests for Hecatonchire, as well as a
framework to create and maintain more tests. It is only a *temporary* construct,
until we setup an automated testing framework. In the meanwhile, we need to
manually operate these tests to stabilize releases and patches.

Existing tests are generic, and use a standard form of configuration files, to
apply with different clusters and setups. Some of them can be expanded.

All the test and poc can be find in the "tests" folder of the libheca repository

Contents
--------
1. compiling the tests
2. configuration files
3. tst (manual page transfer)
4. poc (scaled-out quicksort)
5. kvm (scaled-out kvm/qemu vm)

1. compiling the tests
----------------------
Dependencies:
 * a Hecatonchire Kernel installed, including headers
 * libheca installed, including headers and ldconfig
 * pthread library (for poc test)
 * (optional:) a Hecatonchire QEMU/KVM installed (for kvm test)

To compile, simply do make in the main folder. It will create a ready object
for every test in the batch.

2. configuration files
----------------------
Configuration files for all tests have a standard form. Here is an example:

  size=512
  1=192.168.2.3:4444
  2=192.168.2.4:4444
  #3=192.168.2.5:4444

Each line is in a {key}={value} format, with "#" signifying a comment line. The
integer keys are ids of machines in the cluster, and their attached value is in
{ip}:{port} format. The cluster will be setup accordingly. Machine number 1 is
destined to be the main/computation machine by default, if such a concept exists
in a test. Other parameters, such as "size", may apply to certain tests. It does
not harm to run tests with unused parameters in their configuration files.

Configuration files should be put inside the conf/ folder. 

3. tst (manual page transfer)
-----------------------------
This test assigns one computation node, and one or two memory sponsors. Four
memory regions are registered, and assigned to the memory sponsors. The memory
sponsors dirty the pages, writing their ids to memory. The computation node then
printf()s all these addresses, causing faults to occur. Afterwards, it manually
pushes the pages back to the memory sponsors. The memory sponsors dirty these
pages, and lastly the computation node printf()s again, re-pulling the pages.

3.1 usage
---------
On the computation machine:
 ./tst -m (master) -f [config file name] -k (Kernel Samepage Merging command) -t (Transparent Huge Page command) -p (Pushing and Pulling command) -l [number of loops] -r [time for each rest]
On the memory sponsor/s:
 ./tst -s (slave) -f [config file name] -i [id] -k (Kernel Samepage Merging command) -t (Transparent Huge Page command) -p (Pushing and Pulling command) -l [number of loops] -r [time for each rest]

The cluster will then be setup according to the configuration.

Example:
Computation machine : ./tst -m -f conf/san-tst.conf -p
Memory Sponsor : ./tst -s -f conf/san-tst.conf -i 2 -p

Computation machine : ./tst -m -f conf/san-tst.conf -l 3 -r 3
Memory Sponsor : ./tst -s -f conf/san-tst.conf -i 2 -l 3 -r 3

3.2 walkthrough
---------------
Should "-p" be entered you will require to follow these instructions:

CM := computation machine
MS := memory sponsor

{
 [CM] initialize:           ... press enter
 [MS] initialize {id}:      ... press enter

 [CM] pull all pages:       ... press enter
}
    page contents should be printed, in a format similar to 0:2222222. the
    repeated number is the id of the MS which delivered the page first. if more
    than a single MS exists, some pages should display the id of one MS, while
    others should display the id of the other MS.

{
 [CM] push back pages:      ... press enter
}
    some of the pages should now be pushed to the MSs. no output is expected.

{
 [MS] dirty pages:          ... press enter
}
    MSs dirty the pages which were pushed to them. if more than one MS exists,
    each MS uses a different pattern.

{
 [CM] re-pull pages:        ... press enter
}
    the CM prints the contents of pushed pages, therefore re-pulling them. new
    patterns should be printed - instead of ids, we now have alternating "/" and
    "\" symbols.

{
 [CM] disconnect:           ... press enter
 [MS] disconnect:           ... press enter
}

Otherwise "-k", "-l" or "-t" should run with minimal output.

If all output is as expected, and no crashes occured, the test passed well.

4. poc (scaled-out quicksort)
-----------------------------
This test performs a memory-intensive benchmark over an arbitrary amount of
memory. Some of the memory can be registered to Hecatonchire, effectively
scaling-out the operation. The operation is carried out on a single computation
machine, and other machines may only sponsor memory regions.

The benchmark is a quicksort operation over an array of integers, which is
generated by a constant pattern. The quicksort is parallel (#threads = #cores)
and in-place. It is timed, and integer order is verified upon completion. A
local execution of the benchmark, using sufficient local memory, can be used
as a reference or a baseline.

As a sanity test, the benchmark tests intensive execution of native page fault
and write-out functions.

Depending on your configuration, sorting GBs of data may take an hour or more.
Our recommendation is sorting small amounts of data - 512M-1GB - and simulating
a lack of local memory, using a memory cgroup on the computation machine. The
test therefore needs to be run on the memory-limited cgroup.

4.1 usage
---------
To test locally, without setting up any cluster:
 /poc {memory size}

To test with a Hecatonchire cluster, make sure to add a "size" parameter in
the configuration file, to indicate the total amount of mem (in MB) to be
generated and sorted. Then, on the computation machine:
 ./poc {config file}
On the memory sponsor/s:
 ./poc {config file} {machine id}

4.2 walkthrough
---------------
Explicit output, including times and benchmark success/failure, will be output
on-screen. The only relevant output is on the computation machine.

{
 generating {X} elements ...
}
    the integer array is generated, effectively faulting-in pages from remote
    sponsor/s, writing content and pushing them back when the space is needed
    for other pages.

{
 ... total {X}MB allocated
 ---- bench start ----
  ... > thread set on 0 / {N} (nesting={X})
  ... > thread set on 1 / {N} (nesting={X})
    <snip>
  ... > thread set on {N} / {N} (nesting={X})
}
    this is the heart of the benchmark. quicksort iterates into the array, and
    divides work to different threads. a lot of page faults and write-outs
    should happen right now.

{
 ---- bench end ----
 total {X} seconds ...
  ... payload ok
 Benchmark finished
}
    this is the test output. if the array is unsorted in the end, it will say
    "payload failure" instead of "payload ok". it also outputs the time it took
    to sort, discluding array generation and post-mortem sanity and all other
    stuff.

4.3 test expansion
------------------
This test is a generic wrapper for similar benchmarks, using different payloads
other than quicksort. See the code to easily create and apply other payloads.

5. kvm (scaled-out kvm/qemu vm)
-------------------------------
The batch contains no standard test for using Hecatonchire over KVM/QEMU. In
the past we used one of the standard tests, or the SAP-H benchmark for the SAP
HANA database. In order to test over KVM/QEMU, what needs to be done:
  * setup cgroups on your machines and limit their memory.
  * fire up qemu with correct command-line arguments (example below) on all 
    machines, where one would be depicted as a master and others as slaves. the
    master-slave thing is only relevant for libheca, and does not affect the
    underlying heca mechanism.
  * connect to the vms in any way you see fit, and run a test over them.
  - make sure you create the vm in the memory-limited cgroup!

Example CLI for KVM/QEMU:
 ./x86_64-softmmu/qemu-system-x86_64 -hda /home/roei/hanadb.img -m 18G -heca_master dsmid=1,vminfo=1:192.168.2.5:4444:4445#2:192.168.2.3:4444:4445#,mr=1:19327352832:2# -display vnc=:1 -net tap -net nic -smp 4

